# Week 18: Machine Learning Fundamentals in C

## ğŸ¯ Learning Objectives

Upon completion of this laboratory session, students will be able to:

1. **Remember** the fundamental mathematical operations underlying machine learning algorithms, including matrix multiplication, vector operations and gradient computation
2. **Understand** the distinction between supervised and unsupervised learning paradigms, explaining when each approach is appropriate for different problem types
3. **Apply** gradient descent optimisation to train linear and logistic regression models from scratch in C
4. **Analyse** the computational complexity of different ML algorithms, comparing training time, inference time and memory requirements
5. **Evaluate** model performance using appropriate metrics such as mean squared error, RÂ² score, accuracy and cross-entropy loss
6. **Create** complete implementations of fundamental ML algorithms including K-Nearest Neighbours, K-Means clustering and decision trees

---

## ğŸ“œ Historical Context

The field of machine learning represents one of the most significant intellectual achievements of the twentieth century, emerging from the convergence of statistics, computer science and cognitive psychology. The term "machine learning" was coined by Arthur Samuel in 1959 whilst working at IBM, where he developed a checkers-playing programme that could improve its performance through experienceâ€”a concept that seemed almost magical at the time.

The mathematical foundations trace back to the early work on regression by Carl Friedrich Gauss and Adrien-Marie Legendre in the early 1800s. The method of least squares, developed independently by both mathematicians, provides the theoretical basis for linear regression that remains central to machine learning today. However, the computational implementation of these ideas had to wait nearly two centuries for the advent of electronic computers.

The 1950s and 1960s witnessed the birth of modern neural network research. Frank Rosenblatt's perceptron, introduced in 1958, generated enormous excitement and equally enormous controversy. The subsequent demonstration by Minsky and Papert that single-layer perceptrons could not learn the XOR function led to the first "AI winter"â€”a period of reduced funding and interest. It was not until the 1980s, with the rediscovery and popularisation of the backpropagation algorithm by Rumelhart, Hinton and Williams, that neural networks regained prominence.

### Key Figure: Frank Rosenblatt (1928â€“1971)

Frank Rosenblatt was an American psychologist notable for his work in the field of artificial intelligence. Working at the Cornell Aeronautical Laboratory, Rosenblatt invented the perceptron in 1958, one of the first algorithmically described neural networks. His work was groundbreaking in demonstrating that machines could learn from data rather than being explicitly programmed.

Rosenblatt built the Mark I Perceptron, a machine designed for image recognition that used 400 photocells connected to a network of artificial neurons. The machine could learn to distinguish simple shapes, and Rosenblatt boldly predicted that perceptrons would eventually "be able to walk, talk, see, write, reproduce itself and be conscious of its existence."

Tragically, Rosenblatt died in a boating accident on his 43rd birthday, just as the AI winter was beginning. His pioneering contributions were only fully appreciated decades later when deep learning emerged as a dominant paradigm.

> *"The perceptron is not merely a brain model. It is, in effect, an electronic computer, which can be trained to learn specific tasks."*
> â€” Frank Rosenblatt, *Principles of Neurodynamics* (1962)

### Key Figure: Arthur Samuel (1901â€“1990)

Arthur Lee Samuel was an American pioneer in the field of computer gaming and artificial intelligence. He coined the term "machine learning" in 1959 whilst working at IBM, defining it as the "field of study that gives computers the ability to learn without being explicitly programmed."

Samuel's checkers-playing programme, developed on an IBM 701, was one of the first successful implementations of a self-improving machine. The programme used rote learning and generalisation, storing board positions and their outcomes to improve its play over time. By the early 1960s, the programme had defeated several skilled human players, providing dramatic evidence that machines could indeed learn.

> *"Programming computers to learn from experience should eventually eliminate the need for much of this detailed programming effort."*
> â€” Arthur Samuel, *Some Studies in Machine Learning Using the Game of Checkers* (1959)

---

## ğŸ“š Theoretical Foundations

### 1. Supervised vs Unsupervised Learning

Machine learning algorithms fall into two primary categories based on the nature of the training data and the learning task.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          MACHINE LEARNING PARADIGMS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   SUPERVISED LEARNING                    UNSUPERVISED LEARNING              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚   â€¢ Training data includes labels        â€¢ No labels provided               â”‚
â”‚   â€¢ Goal: learn mapping X â†’ Y            â€¢ Goal: discover structure          â”‚
â”‚   â€¢ Examples:                            â€¢ Examples:                         â”‚
â”‚     - Classification                      - Clustering                       â”‚
â”‚     - Regression                          - Dimensionality reduction         â”‚
â”‚                                           - Anomaly detection                â”‚
â”‚                                                                             â”‚
â”‚   Algorithms:                            Algorithms:                         â”‚
â”‚   â€¢ Linear Regression                    â€¢ K-Means                           â”‚
â”‚   â€¢ Logistic Regression                  â€¢ Hierarchical Clustering           â”‚
â”‚   â€¢ K-Nearest Neighbours                 â€¢ PCA                               â”‚
â”‚   â€¢ Decision Trees                       â€¢ DBSCAN                            â”‚
â”‚   â€¢ Neural Networks                      â€¢ Autoencoders                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Supervised learning** involves training a model on labelled examples where both the input features X and the desired output Y are known. The model learns a function f: X â†’ Y that can generalise to new, unseen examples.

**Unsupervised learning** operates on data without labels, seeking to discover hidden patterns or intrinsic structure. The most common unsupervised task is clustering, where similar data points are grouped together.

### 2. Gradient Descent Optimisation

Gradient descent is the workhorse algorithm for training machine learning models. It iteratively adjusts model parameters to minimise a loss function.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          GRADIENT DESCENT ALGORITHM                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   INITIALISE parameters Î¸ randomly                                          â”‚
â”‚                                                                             â”‚
â”‚   REPEAT until convergence:                                                 â”‚
â”‚       gradient â† âˆ‡_Î¸ Loss(Î¸)           # Compute gradient                  â”‚
â”‚       Î¸ â† Î¸ - Î± Ã— gradient             # Update parameters                  â”‚
â”‚                                                                             â”‚
â”‚   WHERE:                                                                    â”‚
â”‚       Î¸ = model parameters (weights and biases)                            â”‚
â”‚       Î± = learning rate (step size)                                         â”‚
â”‚       Loss(Î¸) = objective function to minimise                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The intuition behind gradient descent is elegantly simple: the gradient of a function points in the direction of steepest ascent. By moving in the opposite direction (negative gradient), we descend towards a minimum.

```
                    Loss
                      â”‚
                  *   â”‚   *
                 *    â”‚    *
                *     â”‚     *
               *      â”‚      *
              *â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€*
             *   â†â”€â”€â”€â”€â”‚â”€â”€â”€â”€â†’   *
            *         â”‚         *
           *â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€*
          *           â”‚           *
         *â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€*  â† Minimum
                 Parameter Î¸
```

### 3. Linear Regression

Linear regression models the relationship between features and a continuous target variable as a linear function.

```
Model:      y = wâ‚€xâ‚€ + wâ‚xâ‚ + ... + wâ‚™xâ‚™ + b
            y = wÂ·x + b   (vector notation)

Loss (MSE): L = (1/n) Ã— Î£áµ¢(yáµ¢ - Å·áµ¢)Â²

Gradients:  âˆ‚L/âˆ‚w = (2/n) Ã— Î£áµ¢ xáµ¢(Å·áµ¢ - yáµ¢)
            âˆ‚L/âˆ‚b = (2/n) Ã— Î£áµ¢ (Å·áµ¢ - yáµ¢)
```

The normal equation provides a closed-form solution:

```
w* = (X^T X)^(-1) X^T y
```

However, gradient descent is preferred when dealing with large datasets or when the matrix inversion is computationally expensive.

### 4. Logistic Regression

Logistic regression extends linear regression to binary classification using the sigmoid function.

```
Sigmoid:    Ïƒ(z) = 1 / (1 + e^(-z))

Model:      P(y=1|x) = Ïƒ(wÂ·x + b)

Properties of sigmoid:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     Ïƒ(z) âˆˆ (0, 1)           Always valid prob  â”‚
            â”‚     Ïƒ(0) = 0.5              Decision boundary  â”‚
            â”‚     Ïƒ(-z) = 1 - Ïƒ(z)        Symmetry property  â”‚
            â”‚     dÏƒ/dz = Ïƒ(z)(1-Ïƒ(z))    Elegant derivative â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Cross-Entropy Loss:
            L = -(1/n) Ã— Î£áµ¢ [yáµ¢ log(páµ¢) + (1-yáµ¢) log(1-páµ¢)]
```

### 5. K-Nearest Neighbours (K-NN)

K-NN is a non-parametric, instance-based learning algorithm that makes predictions based on the K training examples closest to the query point.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              K-NN ALGORITHM                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   PREDICT(query_point, K):                                                  â”‚
â”‚       1. Compute distance to all training points                           â”‚
â”‚       2. Select K nearest neighbours                                        â”‚
â”‚       3. For classification: return majority vote                          â”‚
â”‚          For regression: return mean of K targets                          â”‚
â”‚                                                                             â”‚
â”‚   Distance Metrics:                                                         â”‚
â”‚       Euclidean:  d(a,b) = âˆš(Î£áµ¢(aáµ¢ - báµ¢)Â²)                                 â”‚
â”‚       Manhattan:  d(a,b) = Î£áµ¢|aáµ¢ - báµ¢|                                     â”‚
â”‚       Minkowski:  d(a,b) = (Î£áµ¢|aáµ¢ - báµ¢|áµ–)^(1/p)                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

K-NN has no training phaseâ€”it simply stores all training examples. This makes it a "lazy learner" with O(1) training time but O(nÂ·d) prediction time where n is the number of training samples and d is the dimensionality.

### 6. K-Means Clustering

K-Means partitions n observations into K clusters by iteratively assigning points to the nearest centroid and updating centroids as cluster means.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          K-MEANS (LLOYD'S ALGORITHM)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   INITIALISE centroids Î¼â‚, Î¼â‚‚, ..., Î¼â‚–                                     â”‚
â”‚                                                                             â”‚
â”‚   REPEAT until convergence:                                                 â”‚
â”‚       # Assignment step                                                     â”‚
â”‚       FOR each point xáµ¢:                                                    â”‚
â”‚           cáµ¢ = argminâ±¼ ||xáµ¢ - Î¼â±¼||Â²    # Assign to nearest centroid       â”‚
â”‚                                                                             â”‚
â”‚       # Update step                                                         â”‚
â”‚       FOR each cluster j:                                                   â”‚
â”‚           Î¼â±¼ = (1/|Câ±¼|) Ã— Î£_{iâˆˆCâ±¼} xáµ¢   # Recompute centroid              â”‚
â”‚                                                                             â”‚
â”‚   Objective (Inertia):                                                      â”‚
â”‚       J = Î£â±¼ Î£_{iâˆˆCâ±¼} ||xáµ¢ - Î¼â±¼||Â²     # Sum of squared distances         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7. Decision Trees

Decision trees recursively partition the feature space using axis-aligned splits, creating a tree structure where leaves represent class predictions.

```
Gini Impurity:    G = 1 - Î£áµ¢ páµ¢Â²

Information Gain: IG(parent, split) = G(parent) - Î£â±¼ (|Câ±¼|/|parent|) Ã— G(Câ±¼)

Example Tree:
                    [Xâ‚€ < 3.5?]
                    /          \
                 Yes            No
                  â”‚              â”‚
            [Xâ‚ < 2.0?]     Class 1
            /        \
         Yes          No
          â”‚           â”‚
       Class 0    Class 1
```

### 8. The Perceptron and Neural Networks

The perceptron is the simplest neural network: a single neuron that computes a weighted sum of inputs and applies a step function.

```
Perceptron:     y = step(wÂ·x + b)
                    where step(z) = 1 if z â‰¥ 0, else 0

Perceptron Learning Rule:
                IF prediction â‰  true_label:
                    w â† w + Î· Ã— true_label Ã— x
                    b â† b + Î· Ã— true_label

Limitation:     Cannot learn XOR (not linearly separable)
```

Multi-layer networks overcome this limitation by stacking layers of neurons:

```
          Input      Hidden        Output
           Layer      Layer         Layer

           xâ‚€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â†‘â”‚â†“        â”‚
           xâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â†’ Å·
                      â†‘â”‚â†“        â”‚
           xâ‚‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

       Forward pass:  h = Ïƒ(Wâ‚Â·x + bâ‚)
                      Å· = Ïƒ(Wâ‚‚Â·h + bâ‚‚)
```

---

## ğŸ­ Industrial Applications

### Machine Learning in Database Systems

Modern database systems employ machine learning for query optimisation, cardinality estimation and indexing decisions. Systems like PostgreSQL and MySQL use learned cost models to choose optimal query execution plans.

```c
/* Simplified learned cardinality estimator */
double estimate_cardinality(QueryFeatures *features, LearnedModel *model) {
    double base_estimate = linear_predict(model, features);
    return exp(base_estimate);  /* Log-space prediction */
}
```

### Recommendation Systems

E-commerce platforms and streaming services use collaborative filtering and content-based methods to personalise recommendations.

### Anomaly Detection in Network Security

K-Means and K-NN are used to detect network intrusions by identifying traffic patterns that deviate from normal behaviour.

### Financial Prediction

Logistic regression and decision trees power credit scoring systems, where interpretability is crucial for regulatory compliance.

---

## ğŸ’» Laboratory Exercises

### Exercise 1: House Price Prediction with Linear Regression

Implement a complete linear regression pipeline for predicting house prices based on features such as square footage, number of bedrooms and age.

**Requirements:**

- Load training data from CSV file
- Implement feature normalisation (z-score)
- Train linear regression using gradient descent
- Evaluate using MSE and RÂ² metrics
- Visualise learning curve (loss vs iterations)
- Test on holdout data

**File:** `src/exercise1.c`

### Exercise 2: Iris Classification with K-NN and K-Means

Implement both K-NN classification and K-Means clustering on the Iris dataset, comparing the results.

**Requirements:**

- Load Iris dataset (provided in data/)
- Implement K-NN with configurable K
- Implement K-Means with K-Means++ initialisation
- Compare clustering assignments with true labels
- Calculate accuracy for K-NN
- Calculate cluster purity for K-Means

**File:** `src/exercise2.c`

---

## ğŸ”§ Compilation and Execution

```bash
# Build all targets
make

# Run demonstration
make run

# Run specific example
make run-example

# Run automated tests
make test

# Check for memory leaks
make valgrind

# Run in Docker
docker build -t week-18-ml .
docker run --rm week-18-ml

# Clean build artifacts
make clean

# Display help
make help
```

---

## ğŸ“ Directory Structure

```
18-ml-fundamentals/
â”œâ”€â”€ README.md                       # This documentation
â”œâ”€â”€ Makefile                        # Build automation
â”œâ”€â”€ Dockerfile                      # Container build
â”‚
â”œâ”€â”€ slides/
â”‚   â”œâ”€â”€ presentation-week18.html    # Main lecture slides
â”‚   â””â”€â”€ presentation-comparativ.html # Pseudocode/C/Python comparison
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ example1.c                  # Complete ML demonstration
â”‚   â”œâ”€â”€ exercise1.c                 # Linear regression exercise
â”‚   â””â”€â”€ exercise2.c                 # K-NN and K-Means exercise
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ housing.csv                 # Housing price dataset
â”‚   â”œâ”€â”€ iris.csv                    # Iris flower dataset
â”‚   â””â”€â”€ classification_2d.csv       # 2D classification data
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test1_input.txt
â”‚   â”œâ”€â”€ test1_expected.txt
â”‚   â”œâ”€â”€ test2_input.txt
â”‚   â””â”€â”€ test2_expected.txt
â”‚
â”œâ”€â”€ python_comparison/
â”‚   â”œâ”€â”€ linear_regression_python.py
â”‚   â”œâ”€â”€ knn_python.py
â”‚   â””â”€â”€ kmeans_python.py
â”‚
â”œâ”€â”€ teme/
â”‚   â”œâ”€â”€ homework-requirements.md    # Homework specifications
â”‚   â””â”€â”€ homework-extended.md        # Bonus challenges
â”‚
â””â”€â”€ solution/
    â”œâ”€â”€ exercise1_sol.c
    â”œâ”€â”€ exercise2_sol.c
    â”œâ”€â”€ homework1_sol.c
    â””â”€â”€ homework2_sol.c
```

---

## ğŸ“– Recommended Reading

### Essential

- **Cormen, Leiserson, Rivest, Stein**: *Introduction to Algorithms*, Chapter 35 (Approximation Algorithms) â€” provides theoretical foundations for understanding algorithm complexity
- **Bishop, Christopher M.**: *Pattern Recognition and Machine Learning* â€” comprehensive coverage of probabilistic approaches to ML
- **Hastie, Tibshirani, Friedman**: *The Elements of Statistical Learning* â€” rigorous statistical treatment, available free online

### Historical

- **Rosenblatt, Frank** (1958): "The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain", *Psychological Review* 65(6): 386â€“408
- **Samuel, Arthur** (1959): "Some Studies in Machine Learning Using the Game of Checkers", *IBM Journal* 3(3): 210â€“229
- **Rumelhart, Hinton, Williams** (1986): "Learning representations by back-propagating errors", *Nature* 323: 533â€“536

### Online Resources

- [Visualizing K-Means](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) â€” Interactive K-Means visualisation
- [Neural Network Playground](https://playground.tensorflow.org/) â€” Interactive neural network visualisation
- [MIT OpenCourseWare 6.867](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/) â€” Machine Learning course materials

---

## âœ… Self-Assessment Checklist

After completing this laboratory, verify that you can:

- [ ] Implement matrix multiplication and transpose operations in C
- [ ] Explain gradient descent and implement it for function minimisation
- [ ] Train a linear regression model and evaluate using MSE and RÂ²
- [ ] Implement logistic regression with sigmoid activation
- [ ] Calculate Euclidean and Manhattan distances
- [ ] Implement K-NN classification with configurable K
- [ ] Implement K-Means clustering with proper initialisation
- [ ] Calculate Gini impurity for decision tree splits
- [ ] Explain why perceptrons cannot learn XOR
- [ ] Apply min-max and z-score normalisation to datasets
- [ ] Compare computational complexity of different ML algorithms
- [ ] Choose appropriate algorithms for different problem types

---

## ğŸ’¼ Interview Preparation

Common interview questions on machine learning fundamentals:

1. **Explain the bias-variance trade-off.**
   - Key insight: High bias leads to underfitting, high variance to overfitting

2. **What is the difference between gradient descent and stochastic gradient descent?**
   - Key insight: SGD uses one sample per update, faster but noisier

3. **Why do we need feature normalisation?**
   - Key insight: Features on different scales dominate gradient updates

4. **How do you choose K in K-Means?**
   - Key insight: Elbow method, silhouette score

5. **Can K-NN be used for regression?**
   - Key insight: Yes, by averaging targets of K neighbours

6. **What is the time complexity of K-NN prediction?**
   - Key insight: O(nÂ·d) without optimisation, O(log n) with KD-trees

7. **Explain overfitting in decision trees and how to prevent it.**
   - Key insight: Pruning, max depth, min samples per leaf

---

## ğŸ”— Next Week Preview

**Week 19: Algorithms for IoT and Stream Processing**

Having established the foundations of machine learning in pure C, we turn to the challenging domain of IoT and stream processing. Week 19 explores algorithms designed for constrained environments where memory is limited and data arrives continuously. Topics include circular buffers for sensor data, sliding window algorithms for real-time statistics, Kalman filtering for noisy measurements and anomaly detection in data streams. These techniques combine the ML foundations from this week with practical considerations for embedded systems.

---

## ğŸ”Œ Real Hardware Extension (Optional)

> **Note:** This section is for students who wish to experiment with physical hardware.
> Arduino kits (ESP32, Arduino Due) are available for borrowing from the faculty library.

The ML algorithms implemented in this laboratory can be deployed on microcontrollers for edge inference. The ESP32 provides sufficient computational power for simple models like linear regression and small decision trees.

**Advantages of Edge ML:**

- Reduced latency (no network round-trip)
- Privacy preservation (data stays local)
- Offline operation capability
- Lower bandwidth requirements

**Getting Started:**

1. Install PlatformIO or Arduino IDE
2. Export trained model weights to header files
3. Implement inference-only code (no training)
4. Optimise for fixed-point arithmetic if needed

Example: Deploy a trained linear regression model for real-time temperature prediction based on sensor readings.

---

*Laboratory materials prepared for ADSC Course*
*Academy of Economic Studies - CSIE Bucharest*
*Alternative learning kit for non-formal education*

---

## ğŸ“„ Licence Notice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚                        RESTRICTIVE LICENCE NOTICE                           â”‚
â”‚                                                                             â”‚
â”‚  Â© 2025 All Rights Reserved                                                 â”‚
â”‚                                                                             â”‚
â”‚  This educational material is provided for PERSONAL STUDY ONLY.             â”‚
â”‚                                                                             â”‚
â”‚  PROHIBITED without prior written consent:                                  â”‚
â”‚    â€¢ Publication in any form (print, digital, online)                      â”‚
â”‚    â€¢ Use in formal educational settings (courses, workshops)               â”‚
â”‚    â€¢ Commercial use or redistribution                                       â”‚
â”‚    â€¢ Derivative works for teaching purposes                                â”‚
â”‚                                                                             â”‚
â”‚  For licensing enquiries, contact the author.                               â”‚
â”‚                                                                             â”‚
â”‚  Author: Asistent (pe perioadÄƒ determinatÄƒ) ing. dr. Antonio Clim          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
