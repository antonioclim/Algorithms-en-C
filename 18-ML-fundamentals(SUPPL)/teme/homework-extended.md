# Week 18: Machine Learning Fundamentals in C
## Extended Challenges (Bonus)

---

## ğŸ† Overview

These extended challenges are **optional** and designed for students who wish to deepen their understanding of machine learning algorithms. Each challenge explores advanced concepts beyond the core curriculum.

**Bonus Points**: Each challenge is worth up to 10 bonus points
**Maximum Bonus**: 30 points (best 3 of 5 challenges)

---

## Challenge 1: Mini-batch Stochastic Gradient Descent

### Objective

Implement efficient mini-batch gradient descent with configurable batch sizes and compare convergence behaviour across different batch configurations.

### Background

Full-batch gradient descent computes gradients over the entire dataset, which becomes computationally expensive for large datasets. Stochastic Gradient Descent (SGD) uses single samples, introducing noise but enabling faster iteration. Mini-batch SGD strikes a balance between these extremes.

### Requirements

1. **Implementation**
   - Extend linear regression to support mini-batch updates
   - Implement sample shuffling at each epoch
   - Support configurable batch sizes (1, 16, 32, 64, full)

2. **Convergence Analysis**
   - Track loss at each iteration (not just epoch)
   - Measure wall-clock time to reach target loss
   - Plot learning curves for different batch sizes (ASCII)

3. **Momentum Enhancement** (bonus within bonus)
   - Implement momentum-based SGD
   - Compare convergence with/without momentum
   - Formula: v_t = Î²v_{t-1} + (1-Î²)âˆ‡L; Î¸ = Î¸ - Î±v_t

### Expected Deliverables

```
challenge1/
â”œâ”€â”€ sgd_variants.c      # Main implementation
â”œâ”€â”€ batch_experiment.c  # Batch size comparison
â”œâ”€â”€ Makefile
â””â”€â”€ convergence.txt     # Results and analysis
```

### Evaluation Criteria

| Aspect | Points |
|--------|--------|
| Correct mini-batch implementation | 4 |
| Proper shuffling each epoch | 2 |
| Convergence comparison | 2 |
| Momentum implementation | 2 |

---

## Challenge 2: Regularisation Techniques (L1/L2)

### Objective

Implement L1 (Lasso) and L2 (Ridge) regularisation for linear regression and analyse their effects on model coefficients and generalisation.

### Background

Regularisation adds a penalty term to the loss function to prevent overfitting:
- **L2 (Ridge)**: Loss = MSE + Î»âˆ‘wÂ²áµ¢ â€” shrinks weights uniformly
- **L1 (Lasso)**: Loss = MSE + Î»âˆ‘|wáµ¢| â€” produces sparse solutions

### Requirements

1. **Implementation**
   - Modify linear regression to support regularisation
   - Implement both L1 and L2 penalties
   - Support Elastic Net (combination of L1 and L2)

2. **Gradient Modifications**
   ```
   L2 gradient: âˆ‚L/âˆ‚w = âˆ‚MSE/âˆ‚w + 2Î»w
   L1 gradient: âˆ‚L/âˆ‚w = âˆ‚MSE/âˆ‚w + Î»Â·sign(w)
   ```

3. **Analysis**
   - Create dataset with some irrelevant features
   - Compare coefficients with/without regularisation
   - Demonstrate L1 driving coefficients to exactly zero

4. **Cross-Validation for Î» Selection**
   - Implement grid search over Î» values
   - Plot validation error vs Î» (ASCII)
   - Report optimal Î»

### Expected Deliverables

```
challenge2/
â”œâ”€â”€ regularisation.c     # Main implementation
â”œâ”€â”€ lambda_search.c      # Hyperparameter tuning
â”œâ”€â”€ Makefile
â””â”€â”€ analysis_report.txt  # Coefficient analysis
```

### Evaluation Criteria

| Aspect | Points |
|--------|--------|
| L2 regularisation | 3 |
| L1 regularisation | 3 |
| Sparsity demonstration | 2 |
| Lambda selection via CV | 2 |

---

## Challenge 3: K-Fold Cross-Validation Framework

### Objective

Build a comprehensive cross-validation framework that supports stratified folds, multiple scoring metrics and automated model selection.

### Background

Cross-validation provides robust performance estimates by partitioning data into complementary subsets. K-fold CV rotates through K partitions, using each as a validation set exactly once.

### Requirements

1. **Basic K-Fold**
   - Implement data partitioning into K folds
   - Ensure each sample appears in exactly one fold
   - Support configurable K (default: 5)

2. **Stratified K-Fold**
   - Maintain class proportions in each fold
   - Critical for imbalanced datasets
   - Verify stratification correctness

3. **Nested Cross-Validation**
   - Outer loop: model evaluation
   - Inner loop: hyperparameter tuning
   - Prevents optimistic bias in performance estimates

4. **Multiple Metrics**
   - Support accuracy, precision, recall, F1
   - Report mean Â± standard deviation
   - Statistical significance testing (optional)

### Example Output

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              5-FOLD CROSS-VALIDATION RESULTS                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Model: K-NN (k=5)

Fold Results:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fold     â”‚ Accuracy  â”‚ Precision â”‚ Recall    â”‚ F1 Score  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1        â”‚    0.9333 â”‚    0.9412 â”‚    0.9231 â”‚    0.9320 â”‚
â”‚ 2        â”‚    0.9000 â”‚    0.9091 â”‚    0.8889 â”‚    0.8989 â”‚
â”‚ 3        â”‚    0.9333 â”‚    0.9375 â”‚    0.9375 â”‚    0.9375 â”‚
â”‚ 4        â”‚    0.8667 â”‚    0.8750 â”‚    0.8750 â”‚    0.8750 â”‚
â”‚ 5        â”‚    0.9667 â”‚    0.9688 â”‚    0.9688 â”‚    0.9688 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mean     â”‚    0.9200 â”‚    0.9263 â”‚    0.9187 â”‚    0.9224 â”‚
â”‚ Std      â”‚    0.0365 â”‚    0.0344 â”‚    0.0364 â”‚    0.0351 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Expected Deliverables

```
challenge3/
â”œâ”€â”€ cross_validation.c   # CV framework
â”œâ”€â”€ stratified_cv.c      # Stratified implementation
â”œâ”€â”€ nested_cv.c          # Nested CV for hyperparameter tuning
â”œâ”€â”€ Makefile
â””â”€â”€ cv_results.txt
```

### Evaluation Criteria

| Aspect | Points |
|--------|--------|
| Basic K-fold implementation | 3 |
| Stratified sampling | 3 |
| Multiple metrics | 2 |
| Nested CV | 2 |

---

## Challenge 4: Random Forest (Ensemble Learning)

### Objective

Implement a simplified Random Forest classifier combining multiple decision trees with bagging and random feature selection.

### Background

Random Forests combat overfitting by:
1. **Bagging**: Training each tree on a bootstrap sample
2. **Feature randomness**: Each split considers random feature subset
3. **Majority voting**: Combining predictions from all trees

### Requirements

1. **Decision Tree Extension**
   - Implement max_depth limiting
   - Add min_samples_split parameter
   - Random feature subset at each split (âˆšd features)

2. **Bootstrap Sampling**
   - Sample n points with replacement
   - Approximately 63.2% unique samples per tree
   - Out-of-bag (OOB) samples for validation

3. **Forest Implementation**
   - Support configurable number of trees (default: 100)
   - Implement parallel-ready structure (future pthreads)
   - Majority voting for classification

4. **Feature Importance**
   - Track how often each feature is used for splits
   - Calculate mean decrease in impurity
   - Report ranked feature importance

### Example Output

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              RANDOM FOREST RESULTS                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration:
  Number of trees: 50
  Max depth: 10
  Features per split: 2 (âˆš4)

Performance:
  Training Accuracy: 100.00%
  Test Accuracy:      94.67%
  OOB Accuracy:       93.33%

Feature Importance:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature              â”‚ Importance    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Petal Length         â”‚     0.4523    â”‚
â”‚ Petal Width          â”‚     0.3891    â”‚
â”‚ Sepal Length         â”‚     0.0987    â”‚
â”‚ Sepal Width          â”‚     0.0599    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Expected Deliverables

```
challenge4/
â”œâ”€â”€ decision_tree.c     # Enhanced decision tree
â”œâ”€â”€ random_forest.c     # Forest implementation
â”œâ”€â”€ Makefile
â””â”€â”€ forest_analysis.txt
```

### Evaluation Criteria

| Aspect | Points |
|--------|--------|
| Bootstrap sampling | 2 |
| Random feature selection | 3 |
| Forest aggregation | 3 |
| Feature importance | 2 |

---

## Challenge 5: Naive Bayes Classifier

### Objective

Implement Gaussian Naive Bayes for classification and compare with discriminative approaches (logistic regression, K-NN).

### Background

Naive Bayes is a generative classifier based on Bayes' theorem:

```
P(class|features) âˆ P(features|class) Ã— P(class)
```

The "naive" assumption is that features are conditionally independent given the class, allowing:

```
P(xâ‚,xâ‚‚,...,xâ‚™|class) = âˆP(xáµ¢|class)
```

For continuous features, Gaussian Naive Bayes assumes:
```
P(xáµ¢|class) = N(Î¼_class, ÏƒÂ²_class)
```

### Requirements

1. **Training Phase**
   - Compute class priors P(class) from training data
   - Estimate Î¼ and ÏƒÂ² for each feature per class
   - Handle numerical stability (log probabilities)

2. **Prediction Phase**
   - Compute log P(class) + Î£ log P(xáµ¢|class)
   - Return class with highest posterior probability
   - Provide probability estimates (normalised)

3. **Laplace Smoothing** (optional)
   - Add small constant to prevent zero probabilities
   - Critical for sparse categorical features

4. **Comparison Study**
   - Compare with K-NN and logistic regression
   - Measure training time (Naive Bayes is fast!)
   - Analyse where Naive Bayes excels/fails

### Mathematical Implementation

```c
/* Training: compute statistics */
typedef struct {
    double *class_priors;     /* P(class) for each class */
    double **means;           /* Î¼[class][feature] */
    double **variances;       /* ÏƒÂ²[class][feature] */
    int num_classes;
    int num_features;
} NaiveBayes;

/* Prediction: compute log probability */
double log_gaussian_pdf(double x, double mean, double var) {
    double log_coeff = -0.5 * log(2 * M_PI * var);
    double exponent = -0.5 * (x - mean) * (x - mean) / var;
    return log_coeff + exponent;
}

int predict(NaiveBayes *nb, double *sample) {
    int best_class = 0;
    double best_log_prob = -INFINITY;
    
    for (int c = 0; c < nb->num_classes; c++) {
        double log_prob = log(nb->class_priors[c]);
        for (int f = 0; f < nb->num_features; f++) {
            log_prob += log_gaussian_pdf(
                sample[f], 
                nb->means[c][f], 
                nb->variances[c][f]
            );
        }
        if (log_prob > best_log_prob) {
            best_log_prob = log_prob;
            best_class = c;
        }
    }
    return best_class;
}
```

### Expected Deliverables

```
challenge5/
â”œâ”€â”€ naive_bayes.c       # Main implementation
â”œâ”€â”€ comparison.c        # Comparison with other classifiers
â”œâ”€â”€ Makefile
â””â”€â”€ bayes_analysis.txt
```

### Evaluation Criteria

| Aspect | Points |
|--------|--------|
| Correct probability computation | 4 |
| Log-space implementation | 2 |
| Comparison study | 2 |
| Analysis of assumptions | 2 |

---

## ğŸ“‹ Submission Guidelines

### Format

Submit bonus challenges as additional directories in your homework archive:

```
StudentName_Week18_Homework.zip
â”œâ”€â”€ homework1/
â”œâ”€â”€ homework2/
â”œâ”€â”€ challenge1/        # If attempted
â”œâ”€â”€ challenge3/        # If attempted
â””â”€â”€ challenge5/        # If attempted
```

### Requirements

- All challenges must compile without warnings
- Include Makefile for each challenge
- No memory leaks (Valgrind verified)
- Clear documentation of approach
- Results file with analysis

### Partial Credit

- Partial implementations receive proportional credit
- Document what works and what remains incomplete
- Explain your approach even if not fully implemented

---

## ğŸ“ Learning Outcomes

Completing these challenges demonstrates:

1. **Mini-batch SGD**: Understanding of optimisation landscapes and trade-offs between noise and computation
2. **Regularisation**: Appreciation for bias-variance trade-off and model complexity control
3. **Cross-Validation**: Rigorous model evaluation methodology
4. **Random Forest**: Ensemble learning principles and variance reduction
5. **Naive Bayes**: Generative vs discriminative model comparison

---

## ğŸ’¡ Tips for Success

1. **Start simple**: Get basic version working before optimisations
2. **Test incrementally**: Verify each component independently
3. **Use synthetic data**: Create simple cases with known answers
4. **Compare with Python**: Validate your C implementation against scikit-learn
5. **Profile your code**: Identify bottlenecks before optimising

---

## ğŸ“š Reference Resources

### Mini-batch SGD
- Bottou, L. "Large-Scale Machine Learning with Stochastic Gradient Descent"
- Ruder, S. "An Overview of Gradient Descent Optimization Algorithms"

### Regularisation
- Hastie, Tibshirani & Friedman: Elements of Statistical Learning, Ch. 3
- Murphy: Machine Learning: A Probabilistic Perspective, Ch. 7

### Random Forest
- Breiman, L. "Random Forests" (2001)
- scikit-learn documentation on ensemble methods

### Naive Bayes
- Mitchell: Machine Learning, Ch. 6
- Murphy: Machine Learning: A Probabilistic Perspective, Ch. 3

---

*Extended challenges prepared for ATP Course*
*Academy of Economic Studies - CSIE Bucharest*
*Alternative learning kit for non-formal education*
